/**
 * Chat Handler
 * 
 * å¤„ç†æµå¼ä¼šè¯è¯·æ±‚ï¼Œå®ç°å®Œæ•´çš„ RAG æµç¨‹ï¼š
 * 1. æŸ¥è¯¢å¤„ç†ï¼ˆæ‰©å±•/é‡å†™ï¼‰
 * 2. çŸ¥è¯†åº“æœç´¢
 * 3. ä¸Šä¸‹æ–‡æ„å»º
 * 4. LLM æ¨ç†ï¼ˆæµå¼è¾“å‡ºï¼‰
 *
 * æµå¼è¾“å‡ºæ ¼å¼éµå¾ª AG-UI åè®®ï¼Œä¸ chatkit-middleware çš„ ag-ui-server ä¿æŒä¸€è‡´
 *
 * @module handlers/chat
 */

import { config } from '../config.js';
import { logger } from '../utils/logger.js';
import { queryProcessor } from '../services/query-processor.js';
import { handleSearch, type ProviderSearchRequest } from './search.js';
import { getProviderFactory } from '../services/llm/index.js';
import { chatRateLimiter } from '../services/rate-limiter.js';
import type { LLMInferRequest } from '../services/llm/types.js';

// Import types from generated contract (Contract-First Pattern)
import type { components } from '@knowledgebase/contracts-ts/generated/knowledge-provider';

// Re-export contract types for external use
export type ChatRequest = components['schemas']['ChatRequest'];
export type ChatStreamEvent = components['schemas']['ChatStreamEvent'];
export type SourceReference = components['schemas']['SourceReference'];
export type TokenUsage = components['schemas']['TokenUsage'];

// AG-UI event type (from contract)
type AGUIEventType = ChatStreamEvent['type'];

// Internal event interface (compatible with contract)
interface AGUIEvent {
  type: AGUIEventType;
  threadId: string;
  runId: string;
  messageId?: string;
  role?: 'assistant' | 'user';
  delta?: string;
  error?: string;
  name?: string;
  value?: unknown;
}

/**
 * ç”Ÿæˆå”¯ä¸€ ID
 */
function generateId(prefix: string = 'id'): string {
  return `${prefix}_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`;
}

/**
 * å‘é€ SSE äº‹ä»¶
 */
function sendEvent(
  controller: ReadableStreamDefaultController,
  encoder: TextEncoder,
  event: AGUIEvent
): void {
  controller.enqueue(encoder.encode(`data: ${JSON.stringify(event)}\n\n`));
}

/**
 * æ„å»ºä¸Šä¸‹æ–‡æ–‡æœ¬
 */
function buildContextText(chunks: Array<{ content: string; document_title?: string }>): string {
  if (chunks.length === 0) {
    return 'æš‚æ— ç›¸å…³çŸ¥è¯†åº“å†…å®¹ã€‚';
  }

  return chunks
    .map((chunk, index) => {
      const title = chunk.document_title || `æ¥æº ${index + 1}`;
      return `ã€${title}ã€‘\n${chunk.content}`;
    })
    .join('\n\n---\n\n');
}

/**
 * æ„å»ºç³»ç»Ÿæç¤ºè¯
 */
function buildSystemPrompt(context: string): string {
  const template = config.chat.systemPromptTemplate;
  return template.replace('{context}', context);
}

/**
 * å¤„ç†æµå¼ä¼šè¯è¯·æ±‚
 * 
 * SSE è¾“å‡ºæ ¼å¼éµå¾ª AG-UI åè®®ï¼š
 * - RUN_STARTED
 * - TEXT_MESSAGE_START
 * - TEXT_MESSAGE_CHUNK (å¤šä¸ª)
 * - TEXT_MESSAGE_END
 * - RUN_FINISHED
 */
export async function handleChatStream(request: ChatRequest): Promise<Response> {
  const startTime = Date.now();
  const threadId = request.threadId || generateId('thread');
  const runId = request.runId || generateId('run');
  const messageId = generateId('msg');
  const userId = request.user_id || 'anonymous';
  const searchLimit = request.options?.search_limit || config.chat.defaultSearchLimit;
  const temperature = request.options?.temperature || config.chat.defaultTemperature;
  const maxTokens = request.options?.max_tokens || config.chat.defaultMaxTokens;
  const includeSources = request.options?.include_sources !== false;

  // æ£€æŸ¥ Chat é™æµ
  if (!chatRateLimiter.tryAcquire()) {
    logger.warn({ threadId, runId, userId }, 'âš ï¸ CHAT_RATE_LIMITED');
    return Response.json(
      { 
        error: 'Too many requests. Please try again later.',
        code: 'RATE_LIMITED',
      },
      { status: 429 }
    );
  }

  logger.info({
    threadId,
    runId,
    userId,
    messageLen: request.message.length,
    searchLimit,
  }, 'ğŸ“¥ CHAT_REQUEST | start');

  const stream = new ReadableStream({
    async start(controller) {
      const encoder = new TextEncoder();

      try {
        // Step 1: RUN_STARTED
        sendEvent(controller, encoder, {
          type: 'RUN_STARTED',
          threadId,
          runId,
        });

        // Step 2: æŸ¥è¯¢å¤„ç†ï¼ˆæ‰©å±•/é‡å†™ï¼‰
        const queryResult = await queryProcessor.processQuery(request.message);
        
        logger.info({
          original: request.message.substring(0, 50),
          processed: queryResult.processedQuery.substring(0, 50),
          method: queryResult.method,
          expandedCount: queryResult.expandedQueries?.length || 0,
        }, 'ğŸ” QUERY_PROCESSED');

        // Step 3: æœç´¢çŸ¥è¯†åº“ (ä¼ é€’é¢„å¤„ç†ç»“æœï¼Œé¿å…é‡å¤ LLM è°ƒç”¨)
        const searchRequest: ProviderSearchRequest = {
          user_id: userId,
          query: queryResult.processedQuery,
          limit: searchLimit,
        };

        const searchResult = await handleSearch(searchRequest, {
          preProcessedResult: queryResult,
        });

        logger.info({
          chunksCount: searchResult.chunks.length,
          totalTokens: searchResult.total_tokens,
        }, 'ğŸ“š KNOWLEDGE_SEARCH_COMPLETE');

        // Step 4: å‘é€ CUSTOM äº‹ä»¶ï¼ˆçŸ¥è¯†åº“æ¥æºï¼Œå¦‚æœå¯ç”¨ï¼‰
        if (includeSources && searchResult.chunks.length > 0) {
          const sources: SourceReference[] = searchResult.chunks.map(chunk => ({
            chunk_id: chunk.chunk_id,
            document_title: chunk.document_title,
            content_preview: chunk.content.substring(0, 100) + (chunk.content.length > 100 ? '...' : ''),
            score: chunk.score,
          }));
          
          sendEvent(controller, encoder, {
            type: 'CUSTOM',
            threadId,
            runId,
            name: 'knowledge_sources',
            value: sources,
          });
        }

        // Step 5: æ„å»ºä¸Šä¸‹æ–‡
        const contextText = buildContextText(searchResult.chunks);
        const systemPrompt = buildSystemPrompt(contextText);

        // Step 6: TEXT_MESSAGE_START
        sendEvent(controller, encoder, {
          type: 'TEXT_MESSAGE_START',
          threadId,
          runId,
          messageId,
          role: 'assistant',
        });

        // Step 7: è°ƒç”¨ LLM æµå¼æ¨ç†
        const llmRequest: LLMInferRequest = {
          system_prompt: systemPrompt,
          user_prompt: request.message,
          temperature,
          max_tokens: maxTokens,
        };

        const factory = getProviderFactory();
        const provider = factory.getProvider();

        logger.info({
          provider: provider.id,
          model: config.llm.model,
          contextLen: contextText.length,
        }, 'ğŸ¤– LLM_INFERENCE_START');

        let tokenUsage: TokenUsage | undefined;

        for await (const chunk of provider.inferStream(llmRequest)) {
          if (chunk.type === 'content' && chunk.content) {
            // TEXT_MESSAGE_CHUNK - ä½¿ç”¨ delta å­—æ®µ
            sendEvent(controller, encoder, {
              type: 'TEXT_MESSAGE_CHUNK',
              threadId,
              runId,
              messageId,
              delta: chunk.content,
            });
          } else if (chunk.type === 'done') {
            tokenUsage = chunk.usage;
          } else if (chunk.type === 'error') {
            throw new Error(chunk.error || 'LLM stream error');
          }
        }

        // Step 8: TEXT_MESSAGE_END
        sendEvent(controller, encoder, {
          type: 'TEXT_MESSAGE_END',
          threadId,
          runId,
          messageId,
        });

        // Step 9: å‘é€ CUSTOM äº‹ä»¶ï¼ˆtoken ä½¿ç”¨ç»Ÿè®¡ï¼‰
        if (tokenUsage) {
          sendEvent(controller, encoder, {
            type: 'CUSTOM',
            threadId,
            runId,
            name: 'token_usage',
            value: tokenUsage,
          });
        }

        // Step 10: RUN_FINISHED
        sendEvent(controller, encoder, {
          type: 'RUN_FINISHED',
          threadId,
          runId,
        });

        const duration = Date.now() - startTime;
        logger.info({
          threadId,
          runId,
          userId,
          duration,
          usage: tokenUsage,
        }, 'âœ… CHAT_COMPLETE | success');

      } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        
        // RUN_ERROR
        sendEvent(controller, encoder, {
          type: 'RUN_ERROR',
          threadId,
          runId,
          error: errorMessage,
        });
        
        logger.error({
          threadId,
          runId,
          error: errorMessage,
          stack: error instanceof Error ? error.stack : undefined,
        }, 'âŒ CHAT_ERROR');
      } finally {
        controller.close();
      }
    },
  });

  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
      'X-Accel-Buffering': 'no',
    },
  });
}

/**
 * å¤„ç†éæµå¼ä¼šè¯è¯·æ±‚ï¼ˆåŒæ­¥è¿”å›å®Œæ•´å“åº”ï¼‰
 */
export async function handleChat(request: ChatRequest): Promise<{
  threadId: string;
  runId: string;
  messageId: string;
  response: string;
  sources?: SourceReference[];
  usage?: TokenUsage;
}> {
  const startTime = Date.now();
  const threadId = request.threadId || generateId('thread');
  const runId = request.runId || generateId('run');
  const messageId = generateId('msg');
  const userId = request.user_id || 'anonymous';
  const searchLimit = request.options?.search_limit || config.chat.defaultSearchLimit;
  const temperature = request.options?.temperature || config.chat.defaultTemperature;
  const maxTokens = request.options?.max_tokens || config.chat.defaultMaxTokens;
  const includeSources = request.options?.include_sources !== false;

  logger.info({
    threadId,
    runId,
    userId,
    messageLen: request.message.length,
  }, 'ğŸ“¥ CHAT_REQUEST_SYNC | start');

  try {
    // æŸ¥è¯¢å¤„ç†
    const queryResult = await queryProcessor.processQuery(request.message);

    // æœç´¢çŸ¥è¯†åº“ (ä¼ é€’é¢„å¤„ç†ç»“æœï¼Œé¿å…é‡å¤ LLM è°ƒç”¨)
    const searchRequest: ProviderSearchRequest = {
      user_id: userId,
      query: queryResult.processedQuery,
      limit: searchLimit,
    };
    const searchResult = await handleSearch(searchRequest, {
      preProcessedResult: queryResult,
    });

    // æ„å»ºä¸Šä¸‹æ–‡
    const contextText = buildContextText(searchResult.chunks);
    const systemPrompt = buildSystemPrompt(contextText);

    // è°ƒç”¨ LLM
    const llmRequest: LLMInferRequest = {
      system_prompt: systemPrompt,
      user_prompt: request.message,
      temperature,
      max_tokens: maxTokens,
    };

    const factory = getProviderFactory();
    const provider = factory.getProvider();
    const response = await provider.infer(llmRequest);

    const duration = Date.now() - startTime;
    logger.info({
      threadId,
      runId,
      userId,
      duration,
      usage: response.usage,
    }, 'âœ… CHAT_SYNC_COMPLETE | success');

    // æ„å»ºæ¥æºå¼•ç”¨
    const sources: SourceReference[] | undefined = includeSources && searchResult.chunks.length > 0
      ? searchResult.chunks.map(chunk => ({
          chunk_id: chunk.chunk_id,
          document_title: chunk.document_title,
          content_preview: chunk.content.substring(0, 100) + (chunk.content.length > 100 ? '...' : ''),
          score: chunk.score,
        }))
      : undefined;

    return {
      threadId,
      runId,
      messageId,
      response: response.response_text,
      sources,
      usage: response.usage,
    };
  } catch (error) {
    logger.error({
      threadId,
      runId,
      error: error instanceof Error ? error.message : 'Unknown',
      stack: error instanceof Error ? error.stack : undefined,
    }, 'âŒ CHAT_SYNC_ERROR');
    throw error;
  }
}
